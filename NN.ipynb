{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88fec2cf-ae56-45ef-9110-6e75f9f0a7e8",
   "metadata": {},
   "source": [
    "# Deep Neural Network\n",
    "\n",
    "We attempt to implement a neural network for giving predictions. Appropriate articles explaining:\n",
    "* [Regression with Neural Networks in PyTorch](https://medium.com/@benjamin.phillips22/simple-regression-with-neural-networks-in-pytorch-313f06910379) (Medium)\n",
    "* [How to create a neural network for regression with PyTorch](https://www.machinecurve.com/index.php/2021/07/20/how-to-create-a-neural-network-for-regression-with-pytorch/) (Machinecurve.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bc67089-2e10-454a-b950-024022698513",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from sklearn.metrics import mean_absolute_percentage_error as mape # usage : print(mape(pred, observed))\n",
    "\n",
    "import datetime\n",
    "\n",
    "import torch\n",
    "\n",
    "path = 'C:/Users/sebir/Desktop/M2 Toulouse/DÃ©fi IA/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360d395d-6130-494c-96da-0ec9f6ab4a88",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1ae53eb-219b-484c-a6b4-8d2c1c0f9c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set, X\n",
    "X = pd.read_csv(path + 'X_station_train.csv',\n",
    "                parse_dates=['date'],\n",
    "                infer_datetime_format=True)\n",
    "X.columns = ['number_sta', 'date', 'wind_speed', 'temperature', 'dew_point_temperature', 'humidity', 'wind_direction', 'precipitation', 'Id']\n",
    "\n",
    "# X_agg_filledNA = pd.read_csv(path + 'our_X_agg_filled_nearestNeighbours.csv',\n",
    "#                              parse_dates=['day'],\n",
    "#                              infer_datetime_format=True)\n",
    "\n",
    "# Training set, Y\n",
    "Y = pd.read_csv(path + 'Y_train.csv',\n",
    "                parse_dates=['date'],\n",
    "                infer_datetime_format=True)\n",
    "\n",
    "# Y_filledwithNA = pd.read_csv(path + 'Y_train.csv',\n",
    "#                 parse_dates=['date'],\n",
    "#                 infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d17a70-3851-4dde-b1de-cccf1c751c2d",
   "metadata": {},
   "source": [
    "# Preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e9e1d9-ba3e-4e96-84f2-a70a1a34c502",
   "metadata": {
    "tags": []
   },
   "source": [
    "## `X` hourly to `X_agg`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25792441-10b9-415f-a922-c63cd5bd8315",
   "metadata": {},
   "source": [
    "We have to group the data by data in order to have something comparable with `Y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5f73906-a159-476e-8645-1352c286a1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X['day'] = X['date'].apply(lambda x: x.date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d51217fe-31fb-42f0-8327-24ae14a391c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_agg = X.copy()\n",
    "X_agg = X_agg.groupby(by = ['day', 'number_sta'], as_index = False).agg({\n",
    "    \"number_sta\"            : \"first\",\n",
    "    \"wind_speed\"            : \"median\",\n",
    "    \"temperature\"           : \"median\",\n",
    "    \"dew_point_temperature\" : \"median\",\n",
    "    \"humidity\"              : \"median\",\n",
    "    \"wind_direction\"        : \"median\",\n",
    "    \"precipitation\"         : \"sum\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4425efc-7695-405a-93e0-56f5b8f07479",
   "metadata": {},
   "source": [
    "We firstly drop all the NAs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d033058-8afc-4fed-93b7-cde4bce30fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_agg.dropna(inplace = True)\n",
    "Y = Y.loc[X_agg.index]\n",
    "\n",
    "Y.dropna(inplace = True)\n",
    "X_agg = X_agg.loc[Y.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18c0015-6bc5-49f6-b118-1ed422a870ef",
   "metadata": {},
   "source": [
    "Adding the month before deleting the date, since we have this information in `X_test`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c475ebb-e94b-4cb3-a887-8a1d547ddbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_agg['month'] = X_agg['day'].apply(lambda x: x.month)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359c41cc-0eb2-4b49-b053-882619a877e6",
   "metadata": {},
   "source": [
    "We then delete the Id in `Y` which won't be helpful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "267c832d-f110-4293-8a6c-05d3078b17aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_agg.drop(['day'], axis = 1, inplace = True)\n",
    "Y.drop(['date', 'Id'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6e2bf6-4ad8-411b-96e3-5ed9eb030959",
   "metadata": {},
   "source": [
    "## `X_agg_filledNA`\n",
    "We took `X_agg` and we filled the gaps with data from nearests neighbours. Preprocessing data for coherence with `Y`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa61f86-16c0-4531-bcf6-b514cea6f9d2",
   "metadata": {},
   "source": [
    "Adding the month before deleting the date, since we have this information in `X_test`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8920b4-28b1-4ea0-a313-c26fdad30d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_agg_filledNA['month'] = X_agg_filledNA['day'].apply(lambda x: x.month)\n",
    "X_agg_filledNA.drop('day', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b3aeb4-1ff0-4074-8d16-da4996b05872",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_filledwithNA.dropna(inplace = True)\n",
    "X_agg_filledNA = X_agg_filledNA.loc[Y_filledwithNA.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d472fd-98ef-43b0-bcc7-50dc44ea8543",
   "metadata": {},
   "source": [
    "<center><h1>Neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1fa0d7-7076-46d0-90b3-5d3ae01eb4dd",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9193711a-0665-4996-b89e-34b327b843c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04a5b2a-6264-4148-bfdb-271729a20a9b",
   "metadata": {},
   "source": [
    "## Represent data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4dab0b93-4367-4d8c-b617-938d0112eea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeteoDataset(torch.utils.data.Dataset):\n",
    "    '''\n",
    "    Prepare the dataset for regression\n",
    "    '''\n",
    "\n",
    "    def __init__(self, X, y, scale_data=True):\n",
    "        if not torch.is_tensor(X) and not torch.is_tensor(y):\n",
    "            # Apply scaling if necessary\n",
    "            if scale_data:\n",
    "                X = StandardScaler().fit_transform(X)\n",
    "            self.X = torch.from_numpy(X)\n",
    "            self.y = torch.from_numpy(y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.y[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e64037-c447-4745-aae5-0b4f8403d13f",
   "metadata": {},
   "source": [
    "## Creating the MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c4d9ea0-aeb6-48be-9f48-3cc16a2fe6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    '''\n",
    "    Multilayer Perceptron for regression.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(8, 64), # size of the input is here\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Forward pass\n",
    "        '''\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b18d65a-5f18-4b8c-a799-004010142092",
   "metadata": {},
   "source": [
    "## Load data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9071a1b-40ae-45c9-8ddd-358f659dac5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x20f593d5210>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "51e4bf5a-39f1-40df-96f9-bfcad909b814",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = X_agg.to_numpy()\n",
    "y = Y['Ground_truth'].to_numpy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0005b80e-8691-422c-9805-8491b09f3221",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MeteoDataset(X_train, y_train)\n",
    "trainloader = torch.utils.data.DataLoader(dataset,\n",
    "                                          batch_size = 100,\n",
    "                                          shuffle = True,\n",
    "                                          # num_workers=1\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5723fe22-9d49-4033-9b8d-3185aea2096d",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cd872a6c-3628-4122-b0fb-d6edf8cdb5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the MLP\n",
    "mlp = MLP()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6555669-cc76-4442-9d86-fcebde3d8e65",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d045f950-0b82-4bc0-b595-eec9963e9997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n",
      "Loss after mini-batch     1: 17.583\n",
      "Loss after mini-batch    11: 247.920\n",
      "Loss after mini-batch    21: 164.474\n",
      "Loss after mini-batch    31: 179.264\n",
      "Loss after mini-batch    41: 202.696\n",
      "Loss after mini-batch    51: 220.942\n",
      "Loss after mini-batch    61: 191.056\n",
      "Loss after mini-batch    71: 219.627\n",
      "Loss after mini-batch    81: 191.281\n",
      "Loss after mini-batch    91: 234.490\n",
      "Loss after mini-batch   101: 282.328\n",
      "Loss after mini-batch   111: 195.614\n",
      "Loss after mini-batch   121: 252.681\n",
      "Loss after mini-batch   131: 174.947\n",
      "Loss after mini-batch   141: 220.549\n",
      "Loss after mini-batch   151: 200.663\n",
      "Loss after mini-batch   161: 207.145\n",
      "Loss after mini-batch   171: 218.879\n",
      "Loss after mini-batch   181: 215.080\n",
      "Loss after mini-batch   191: 204.134\n",
      "Loss after mini-batch   201: 217.671\n",
      "Loss after mini-batch   211: 232.037\n",
      "Loss after mini-batch   221: 238.103\n",
      "Loss after mini-batch   231: 163.512\n",
      "Loss after mini-batch   241: 229.310\n",
      "Loss after mini-batch   251: 186.246\n",
      "Loss after mini-batch   261: 290.626\n",
      "Loss after mini-batch   271: 181.927\n",
      "Loss after mini-batch   281: 166.526\n",
      "Loss after mini-batch   291: 156.326\n",
      "Loss after mini-batch   301: 225.643\n",
      "Loss after mini-batch   311: 170.455\n",
      "Loss after mini-batch   321: 238.572\n",
      "Loss after mini-batch   331: 220.678\n",
      "Loss after mini-batch   341: 150.909\n",
      "Loss after mini-batch   351: 191.148\n",
      "Loss after mini-batch   361: 184.137\n",
      "Loss after mini-batch   371: 187.815\n",
      "Loss after mini-batch   381: 187.704\n",
      "Loss after mini-batch   391: 225.602\n",
      "Loss after mini-batch   401: 178.208\n",
      "Loss after mini-batch   411: 183.810\n",
      "Loss after mini-batch   421: 165.030\n",
      "Loss after mini-batch   431: 149.442\n",
      "Loss after mini-batch   441: 175.744\n",
      "Loss after mini-batch   451: 164.134\n",
      "Loss after mini-batch   461: 173.032\n",
      "Loss after mini-batch   471: 226.296\n",
      "Loss after mini-batch   481: 136.655\n",
      "Loss after mini-batch   491: 148.487\n",
      "Loss after mini-batch   501: 157.252\n",
      "Loss after mini-batch   511: 184.999\n",
      "Loss after mini-batch   521: 168.239\n",
      "Loss after mini-batch   531: 187.997\n",
      "Loss after mini-batch   541: 150.976\n",
      "Loss after mini-batch   551: 208.859\n",
      "Loss after mini-batch   561: 179.108\n",
      "Loss after mini-batch   571: 145.117\n",
      "Loss after mini-batch   581: 213.618\n",
      "Loss after mini-batch   591: 122.849\n",
      "Starting epoch 2\n",
      "Loss after mini-batch     1: 10.827\n",
      "Loss after mini-batch    11: 194.401\n",
      "Loss after mini-batch    21: 134.043\n",
      "Loss after mini-batch    31: 166.406\n",
      "Loss after mini-batch    41: 170.258\n",
      "Loss after mini-batch    51: 169.701\n",
      "Loss after mini-batch    61: 198.434\n",
      "Loss after mini-batch    71: 156.014\n",
      "Loss after mini-batch    81: 173.175\n",
      "Loss after mini-batch    91: 169.422\n",
      "Loss after mini-batch   101: 152.437\n",
      "Loss after mini-batch   111: 147.273\n",
      "Loss after mini-batch   121: 195.846\n",
      "Loss after mini-batch   131: 169.408\n",
      "Loss after mini-batch   141: 169.467\n",
      "Loss after mini-batch   151: 189.786\n",
      "Loss after mini-batch   161: 160.020\n",
      "Loss after mini-batch   171: 177.201\n",
      "Loss after mini-batch   181: 186.818\n",
      "Loss after mini-batch   191: 153.116\n",
      "Loss after mini-batch   201: 154.354\n",
      "Loss after mini-batch   211: 192.082\n",
      "Loss after mini-batch   221: 166.521\n",
      "Loss after mini-batch   231: 180.094\n",
      "Loss after mini-batch   241: 246.273\n",
      "Loss after mini-batch   251: 198.084\n",
      "Loss after mini-batch   261: 235.649\n",
      "Loss after mini-batch   271: 149.167\n",
      "Loss after mini-batch   281: 136.631\n",
      "Loss after mini-batch   291: 154.133\n",
      "Loss after mini-batch   301: 166.736\n",
      "Loss after mini-batch   311: 163.114\n",
      "Loss after mini-batch   321: 174.315\n",
      "Loss after mini-batch   331: 156.344\n",
      "Loss after mini-batch   341: 199.120\n",
      "Loss after mini-batch   351: 142.157\n",
      "Loss after mini-batch   361: 185.684\n",
      "Loss after mini-batch   371: 193.480\n",
      "Loss after mini-batch   381: 120.457\n",
      "Loss after mini-batch   391: 178.793\n",
      "Loss after mini-batch   401: 177.010\n",
      "Loss after mini-batch   411: 202.625\n",
      "Loss after mini-batch   421: 178.862\n",
      "Loss after mini-batch   431: 209.533\n",
      "Loss after mini-batch   441: 186.939\n",
      "Loss after mini-batch   451: 197.949\n",
      "Loss after mini-batch   461: 145.193\n",
      "Loss after mini-batch   471: 164.307\n",
      "Loss after mini-batch   481: 142.552\n",
      "Loss after mini-batch   491: 149.432\n",
      "Loss after mini-batch   501: 139.283\n",
      "Loss after mini-batch   511: 173.249\n",
      "Loss after mini-batch   521: 205.249\n",
      "Loss after mini-batch   531: 201.998\n",
      "Loss after mini-batch   541: 215.105\n",
      "Loss after mini-batch   551: 133.955\n",
      "Loss after mini-batch   561: 212.713\n",
      "Loss after mini-batch   571: 195.507\n",
      "Loss after mini-batch   581: 135.389\n",
      "Loss after mini-batch   591: 123.179\n",
      "Starting epoch 3\n",
      "Loss after mini-batch     1: 17.000\n",
      "Loss after mini-batch    11: 181.843\n",
      "Loss after mini-batch    21: 150.106\n",
      "Loss after mini-batch    31: 210.852\n",
      "Loss after mini-batch    41: 152.612\n",
      "Loss after mini-batch    51: 159.842\n",
      "Loss after mini-batch    61: 189.676\n",
      "Loss after mini-batch    71: 158.043\n",
      "Loss after mini-batch    81: 188.106\n",
      "Loss after mini-batch    91: 176.508\n",
      "Loss after mini-batch   101: 176.774\n",
      "Loss after mini-batch   111: 163.693\n",
      "Loss after mini-batch   121: 161.727\n",
      "Loss after mini-batch   131: 190.583\n",
      "Loss after mini-batch   141: 197.566\n",
      "Loss after mini-batch   151: 148.478\n",
      "Loss after mini-batch   161: 189.679\n",
      "Loss after mini-batch   171: 191.102\n",
      "Loss after mini-batch   181: 144.967\n",
      "Loss after mini-batch   191: 206.801\n",
      "Loss after mini-batch   201: 162.121\n",
      "Loss after mini-batch   211: 139.029\n",
      "Loss after mini-batch   221: 171.385\n",
      "Loss after mini-batch   231: 183.938\n",
      "Loss after mini-batch   241: 157.619\n",
      "Loss after mini-batch   251: 180.094\n",
      "Loss after mini-batch   261: 157.163\n",
      "Loss after mini-batch   271: 145.434\n",
      "Loss after mini-batch   281: 229.850\n",
      "Loss after mini-batch   291: 170.141\n",
      "Loss after mini-batch   301: 181.913\n",
      "Loss after mini-batch   311: 163.699\n",
      "Loss after mini-batch   321: 177.312\n",
      "Loss after mini-batch   331: 198.415\n",
      "Loss after mini-batch   341: 139.286\n",
      "Loss after mini-batch   351: 155.128\n",
      "Loss after mini-batch   361: 175.078\n",
      "Loss after mini-batch   371: 186.233\n",
      "Loss after mini-batch   381: 155.084\n",
      "Loss after mini-batch   391: 165.201\n",
      "Loss after mini-batch   401: 170.332\n",
      "Loss after mini-batch   411: 144.733\n",
      "Loss after mini-batch   421: 148.768\n",
      "Loss after mini-batch   431: 122.021\n",
      "Loss after mini-batch   441: 142.729\n",
      "Loss after mini-batch   451: 187.998\n",
      "Loss after mini-batch   461: 170.031\n",
      "Loss after mini-batch   471: 202.610\n",
      "Loss after mini-batch   481: 155.293\n",
      "Loss after mini-batch   491: 145.849\n",
      "Loss after mini-batch   501: 197.819\n",
      "Loss after mini-batch   511: 249.094\n",
      "Loss after mini-batch   521: 131.863\n",
      "Loss after mini-batch   531: 156.503\n",
      "Loss after mini-batch   541: 192.543\n",
      "Loss after mini-batch   551: 145.031\n",
      "Loss after mini-batch   561: 213.568\n",
      "Loss after mini-batch   571: 176.990\n",
      "Loss after mini-batch   581: 153.962\n",
      "Loss after mini-batch   591: 149.550\n",
      "Training process has finished.\n"
     ]
    }
   ],
   "source": [
    "# Run the training loop\n",
    "for epoch in range(0, 3):\n",
    "    print(f'Starting epoch {epoch+1}')\n",
    "\n",
    "    # Set current loss value\n",
    "    current_loss = 0.0\n",
    "\n",
    "    # Iterate over the DataLoader for training data\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "\n",
    "        # Get and prepare inputs\n",
    "        inputs, targets = data\n",
    "        inputs, targets = inputs.float(), targets.float()\n",
    "        targets = targets.reshape((targets.shape[0], 1))\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Perform forward pass\n",
    "        outputs = mlp(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_function(outputs, targets)\n",
    "\n",
    "        # Perform backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Perform optimization\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        current_loss += loss.item()\n",
    "        if i % 10 == 0:\n",
    "            print('Loss after mini-batch %5d: %.3f' % (i + 1, current_loss))\n",
    "            current_loss = 0.0\n",
    "\n",
    "# Process is complete.\n",
    "print('Training process has finished.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b753a0-a30b-44ba-85a8-f6ef7348dade",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4a9aa272-f98d-4009-ac2b-1ed4a39b8f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = MeteoDataset(X_test, y_test)\n",
    "testloader = torch.utils.data.DataLoader(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d4d94236-8275-48f1-a9d7-d124bbe45a92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.425169944763184"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a1bc8198-8d2e-4c3a-8e46-e799638c0a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = []\n",
    "for i, data in enumerate(testloader, 0):\n",
    "\n",
    "        # Get and prepare inputs\n",
    "        inputs, targets = data\n",
    "        inputs, targets = inputs.float(), targets.float()\n",
    "        targets = targets.reshape((targets.shape[0], 1))\n",
    "\n",
    "        # Perform forward pass\n",
    "        outputs.append(mlp(inputs).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d2daaf42-8b8e-4421-9ca2-32ba2f8cd86e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3745001700824642"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(outputs, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0a3a9b-f63f-481e-8c9f-63dc1ef2f1e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLenv",
   "language": "python",
   "name": "dlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
